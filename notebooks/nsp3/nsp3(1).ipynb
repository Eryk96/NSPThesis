{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confident-sound",
   "metadata": {
    "papermill": {
     "duration": 0.025759,
     "end_time": "2021-03-01T13:43:26.353067",
     "exception": false,
     "start_time": "2021-03-01T13:43:26.327308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NetSurfP 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-injury",
   "metadata": {
    "papermill": {
     "duration": 0.02218,
     "end_time": "2021-03-01T13:43:26.398364",
     "exception": false,
     "start_time": "2021-03-01T13:43:26.376184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The purpose of this notebook is to improve NetSurfP 2.0 to use pretrained model embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-yesterday",
   "metadata": {
    "papermill": {
     "duration": 0.022485,
     "end_time": "2021-03-01T13:43:26.443952",
     "exception": false,
     "start_time": "2021-03-01T13:43:26.421467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Load libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-radio",
   "metadata": {
    "papermill": {
     "duration": 0.022632,
     "end_time": "2021-03-01T13:43:26.488658",
     "exception": false,
     "start_time": "2021-03-01T13:43:26.466026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The different libraries are initialized and pytorch is either configured to use the CPU or an available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "valid-greek",
   "metadata": {
    "papermill": {
     "duration": 1.734644,
     "end_time": "2021-03-01T13:43:28.246424",
     "exception": false,
     "start_time": "2021-03-01T13:43:26.511780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import optuna\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# custom packages in parent folder\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"UTILS\"] = \"/home/projects/ht3_aim/people/erikie/NSPThesis/notebooks/utils\"\n",
    "\n",
    "computerome = os.getenv(\"UTILS\", \"..\")\n",
    "sys.path.append(computerome)\n",
    "\n",
    "from early_stopping import EarlyStopping\n",
    "from masked_batch_norm import MaskedBatchNorm1d\n",
    "\n",
    "# debugging\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-arena",
   "metadata": {},
   "source": [
    "Setup CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "postal-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-excellence",
   "metadata": {
    "papermill": {
     "duration": 0.023504,
     "end_time": "2021-03-01T13:43:28.293546",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.270042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Preparation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-prediction",
   "metadata": {
    "papermill": {
     "duration": 0.023015,
     "end_time": "2021-03-01T13:43:28.339768",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.316753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each dataset is compressed as a numpy zip and the dimensions of the dataset are as following: \n",
    "\n",
    "**[sequence, position, label]**\n",
    "\n",
    "The labels are as following in the data (shown by index):\n",
    "- [0:20] Amino Acids (sparse encoding) (Unknown residues are stored as an all-zero vector)\n",
    "- [20:50] hmm profile\n",
    "- [50] Seq mask (1 = seq, 0 = empty)\n",
    "- [51] Disordered mask (0 = disordered, 1 = ordered)\n",
    "- [52] Evaluation mask (For CB513 dataset, 1 = eval, 0 = ignore)\n",
    "- [53] ASA (isolated)\n",
    "- [54] ASA (complexed)\n",
    "- [55] RSA (isolated)\n",
    "- [56] RSA (complexed)\n",
    "- [57:65] Q8 GHIBESTC (Q8 -> Q3: HHHEECCC)\n",
    "- [65:67] Phi+Psi\n",
    "- [67] ASA_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-powder",
   "metadata": {
    "papermill": {
     "duration": 0.023515,
     "end_time": "2021-03-01T13:43:28.386714",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.363199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "romance-repair",
   "metadata": {
    "papermill": {
     "duration": 0.037209,
     "end_time": "2021-03-01T13:43:28.447012",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.409803",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_dir = \"../../data/nsp3/training_data/\"\n",
    "\n",
    "train_hhblits = np.load(data_dir + \"small_esm1b_train_hhblits.npz\")\n",
    "#CB513_hhblits = np.load(data_dir + \"CB513_HHblits.npz\")\n",
    "#TS115_hhblits = np.load(data_dir + \"TS115_HHblits.npz\")\n",
    "\n",
    "CASP12_hhblits = np.load(data_dir + \"esm1b_CASP12_HHblits.npz\")\n",
    "\n",
    "#train_mmseqs = np.load(\"../../data/nsp2/training_data/Train_MMseqs.npz\")\n",
    "#CB513_mmseqs = np.load(\"../../data/nsp2/training_data/CB513_MMseqs.npz\")\n",
    "#TS115_mmseqs = np.load(\"../../data/nsp2/training_data/TS115_MMseqs.npz\")\n",
    "#CASP12_mmseqs = np.load(\"../../data/nsp2/training_data/CASP12_MMseqs.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-sheet",
   "metadata": {
    "papermill": {
     "duration": 0.023226,
     "end_time": "2021-03-01T13:43:28.493482",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.470256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Custom Data loader class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-effect",
   "metadata": {
    "papermill": {
     "duration": 0.022186,
     "end_time": "2021-03-01T13:43:28.538152",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.515966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A data loader class is created to load the NSP data into a DataLoader object. The DataLoader is a pytorch object, thats used to feed the data as batches. Which will be usefull when training and evaluating the NSP model. The NSPData class divides the dataset into input data and target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gross-cambodia",
   "metadata": {
    "papermill": {
     "duration": 0.030064,
     "end_time": "2021-03-01T13:43:28.591102",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.561038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NSPData(Dataset):\n",
    "    def __init__(self, dataset, indices = False):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            X (np.array): The array that contains the training data\n",
    "            y (np.array): The array that contains the test data\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = torch.tensor(dataset[:, :, :20]).float()\n",
    "        self.embeddings = torch.tensor(dataset[:, :, 68:]).float()\n",
    "        self.targets = torch.tensor(dataset[:, :, 50:68]).float()\n",
    "        self.lengths = torch.tensor([sum(target[:, 0] == 1) for target in self.targets])\n",
    "        \n",
    "        # add the one hot encoding to the embeddings\n",
    "        self.embeddings = torch.cat([self.data[:, :, :20], self.embeddings], dim=2)\n",
    "        \n",
    "        self.unknown_nucleotide_mask()\n",
    "        \n",
    "    def unknown_nucleotide_mask(self):\n",
    "        \"\"\" Augments the target with a unknown nucleotide mask\n",
    "            by finding entries that don't have any residue\n",
    "        \"\"\"\n",
    "        \n",
    "        # creates a mask based on the one hot encoding\n",
    "        unknown_nucleotides = torch.max(self.data[:, :, :20], dim=2)\n",
    "        unknown_nucleotides = unknown_nucleotides[0].unsqueeze(2)\n",
    "        \n",
    "        # merge the mask to first position of the targets\n",
    "        self.targets = torch.cat([self.targets, unknown_nucleotides], dim=2)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns train and test data at an index\n",
    "        Args:\n",
    "            index (int): Index at the array\n",
    "        \"\"\"\n",
    "        X = self.embeddings[index]\n",
    "        y = self.targets[index]\n",
    "        lengths = self.lengths[index]\n",
    "        \n",
    "        return X, y, lengths\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the data\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-sweden",
   "metadata": {},
   "source": [
    "# Holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-doubt",
   "metadata": {
    "papermill": {
     "duration": 0.023327,
     "end_time": "2021-03-01T13:43:28.639797",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.616470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It is important to have a training and validation set when training to evaluate the training performance. Therefore, a function that splits the dataset into training and evaluation is created for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nasty-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(batch_size, dataset):\n",
    "    \"\"\" Splits the dataset into train and validation\n",
    "    Args:\n",
    "        batch_size (int): size of each batch\n",
    "        dataset (np.array): dataset containing training data\n",
    "        validation_fraction (float): the size of the validation set as a fraction\n",
    "    \"\"\"\n",
    "    \n",
    "    num_train = len(dataset)\n",
    "    train_indices = np.array(range(num_train))\n",
    "    validation_indices = np.random.choice(train_indices, int(num_train*0.05), replace=False)\n",
    "    \n",
    "    train_indices = np.delete(train_indices, validation_indices)\n",
    "    \n",
    "    # subset the dataset\n",
    "    train_idx, valid_idx = train_indices, validation_indices\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    training_set = DataLoader(NSPData(dataset), sampler=train_sampler, batch_size=batch_size)\n",
    "    validation_set = DataLoader(NSPData(dataset), sampler=valid_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return training_set, validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-motorcycle",
   "metadata": {
    "papermill": {
     "duration": 0.023201,
     "end_time": "2021-03-01T13:43:28.840119",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.816918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Instanciate data into the data loader class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-tunnel",
   "metadata": {
    "papermill": {
     "duration": 0.023153,
     "end_time": "2021-03-01T13:43:28.886618",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.863465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The training datasets are splitted into training and validation for training. Whereas the test sets are simply loaded, since they are independent datasets. The batch sizes is set to 15. The default training/validation split is 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "medium-radical",
   "metadata": {
    "papermill": {
     "duration": 0.027929,
     "end_time": "2021-03-01T13:43:28.938050",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.910121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unexpected-polyester",
   "metadata": {
    "papermill": {
     "duration": 338.922507,
     "end_time": "2021-03-01T13:49:07.885865",
     "exception": false,
     "start_time": "2021-03-01T13:43:28.963358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training and validation sets\n",
    "#train_hhblits = DataLoader(NSPData(train_hhblits), batch_size=batch_size, shuffle=True) \n",
    "train_hhblits = split_dataset(batch_size, train_hhblits['data'])\n",
    "#train_mmseqs = DataLoader(NSPData(train_mmseqs), batch_size=batch_size) #split_dataset(batch_size, train_mmseqs)\n",
    "\n",
    "# test sets\n",
    "#CB513_hhblits = DataLoader(NSPData(CB513_hhblits), batch_size=len(CB513_hhblits['data']))\n",
    "#TS115_hhblits = DataLoader(NSPData(TS115_hhblits), batch_size=len(TS115_hhblits['data']))\n",
    "CASP12_hhblits = DataLoader(NSPData(CASP12_hhblits['data']), batch_size=len(CASP12_hhblits['data']))\n",
    "\n",
    "#CB513_mmseqs = DataLoader(NSPData(CB513_mmseqs), batch_size=batch_size)\n",
    "#TS115_mmseqs = DataLoader(NSPData(TS115_mmseqs), batch_size=batch_size)\n",
    "#CASP12_mmseqs = DataLoader(NSPData(CASP12_mmseqs), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-taiwan",
   "metadata": {
    "papermill": {
     "duration": 0.024897,
     "end_time": "2021-03-01T13:49:07.999450",
     "exception": false,
     "start_time": "2021-03-01T13:49:07.974553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. The NSP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-arcade",
   "metadata": {
    "papermill": {
     "duration": 0.025355,
     "end_time": "2021-03-01T13:49:08.050664",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.025309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The input layer consists of one-hot encoded sequences (20 features amino acids) and a full HMM profile (20 features for amino acid profile, 7 features of state transition probabilities and 3 features for local alignment diversity) The input layer is followed by two layers of 1D CNN layers, that consist of 32 filters with a size of 129 and 257. Whereas the output of the last 1D CNN is concatenated with the initial input features. These residuals connections are used to achieve a better backpropagation without gradient degradation and a deeper network. The concatenated input+residuals is then applied to 2 bidirectional long  short-term memory (LSTM) layers with 1024 nodes, that outputs 2048 hidden neurons. The hidden neurons output is input to a fully connected (FC) layer to predict the 18 classes (RSA, SS8, SS3, φ, ψ, and disorder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "economic-arizona",
   "metadata": {
    "papermill": {
     "duration": 0.094624,
     "end_time": "2021-03-01T13:49:08.169380",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.074756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NSP_Network(nn.Module):\n",
    "    def __init__(self, init_channels, n_hidden, cnn_output, params):\n",
    "        \"\"\" Initializes the model with the required layers\n",
    "        Args:\n",
    "            init_channels: The size of the incoming feature vector\n",
    "            n_hidden: The amount of hidden neurons in the bidirectional lstm\n",
    "        \"\"\"\n",
    "        super(NSP_Network, self).__init__()\n",
    "\n",
    "        # CNN block\n",
    "        self.conv1 = nn.Sequential(*[\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv1d(in_channels=init_channels, out_channels=cnn_output, kernel_size=129, padding=64),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "\n",
    "        self.conv2 = nn.Sequential(*[\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv1d(in_channels=init_channels, out_channels=cnn_output, kernel_size=257, padding=128),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "        \n",
    "        self.batch_norm = MaskedBatchNorm1d(cnn_output*2+init_channels)\n",
    "        \n",
    "        decrease_value = 512\n",
    "        self.decrease = nn.Linear(in_features=cnn_output*2+init_channels, out_features=decrease_value)\n",
    "\n",
    "        # LSTM block\n",
    "        #self.lstm = nn.LSTM(input_size=init_channels+cnn_output*2, hidden_size=n_hidden, batch_first=True, \\\n",
    "        #                    num_layers=2, bidirectional=True, dropout=0.5)\n",
    "        \n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=decrease_value, nhead=8, dropout=0.5)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=params['num_layers_encoder'])\n",
    "        \n",
    "        # add dropout to last layer\n",
    "        self.lstm_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # output block\n",
    "        self.ss8 = nn.Sequential(*[\n",
    "            nn.Linear(in_features=decrease_value, out_features=8),\n",
    "            #nn.Softmax(),\n",
    "        ])\n",
    "        self.ss3 = nn.Sequential(*[\n",
    "            nn.Linear(in_features=decrease_value, out_features=3),\n",
    "            #nn.Softmax(),\n",
    "        ])     \n",
    "        self.disorder = nn.Sequential(*[\n",
    "            nn.Linear(in_features=decrease_value, out_features=2),\n",
    "            #nn.Softmax(),\n",
    "        ])\n",
    "        self.rsa = nn.Sequential(*[\n",
    "            nn.Linear(in_features=decrease_value, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        self.phi = nn.Sequential(*[\n",
    "            nn.Linear(in_features=decrease_value, out_features=2),\n",
    "            nn.Tanh()\n",
    "        ])\n",
    "        self.psi = nn.Sequential(*[\n",
    "            nn.Linear(in_features=decrease_value, out_features=2),\n",
    "            nn.Tanh()\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, t, mask):\n",
    "        \"\"\" Forwards the input through each layer in the model\n",
    "        Args:\n",
    "            x: input data containing sequences\n",
    "            lengths: list containing each sequence length\n",
    "            mask: data containing the sequence padding mask\n",
    "        \"\"\"\n",
    "    \n",
    "        _, length, _ = x.size()\n",
    "        \n",
    "        # calculate the residuals\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        r1 = self.conv1(x)\n",
    "        r2 = self.conv2(x)\n",
    "                \n",
    "        # concatenate channels from residuals and input\n",
    "        x = torch.cat([x, r1, r2], dim=1)\n",
    "                                 \n",
    "        # normalize\n",
    "        x = self.batch_norm(x, mask.unsqueeze(1))\n",
    "        \n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.decrease(x)\n",
    "        \n",
    "        # calculate double layer bidirectional lstm\n",
    "        \n",
    "        #x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        #x, _ = self.lstm(x)\n",
    "        #x, _ = pad_packed_sequence(x, total_length=length, batch_first=True)\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        x = torch.nan_to_num(x)\n",
    "        \n",
    "        x = self.lstm_dropout(x)\n",
    "        \n",
    "        # hidden neurons to classes\n",
    "        ss8 = self.ss8(x)\n",
    "        ss3 = self.ss3(x)\n",
    "        disorder = self.disorder(x)\n",
    "        rsa = self.rsa(x)\n",
    "        phi = self.phi(x)\n",
    "        psi = self.psi(x)\n",
    "\n",
    "        return [ss8, ss3, disorder, rsa, phi, psi]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-galaxy",
   "metadata": {
    "papermill": {
     "duration": 0.023519,
     "end_time": "2021-03-01T13:49:08.216731",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.193212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-mandate",
   "metadata": {
    "papermill": {
     "duration": 0.024103,
     "end_time": "2021-03-01T13:49:08.264210",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.240107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To train the model, loss functions are required. The tasks SS8, SS3 and disorder uses cross entropy loss function and RSA, φ and ψ use mean squared error loss.\n",
    "\n",
    "Weights were adjusted so each loss contribution was approximately equal by using homoscedastic uncertainty and optimizing the log variances for each prediction. (https://arxiv.org/abs/1705.07115)\n",
    "\n",
    "In the forwarding of the MultiTaskLoss, filters are used to remove extrapolated zeros or disordered regions from the calculated losses.\n",
    "\n",
    "The indexes for the labels are described in the data preparation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "respected-automation",
   "metadata": {
    "papermill": {
     "duration": 0.037468,
     "end_time": "2021-03-01T13:49:08.324916",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.287448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\" Weighs multiple loss functions by considering the \n",
    "        homoscedastic uncertainty of each task \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        \n",
    "    def mse(self, outputs, labels, mask):\n",
    "        loss = torch.square(outputs - labels) * mask\n",
    "        return torch.sum(loss) / torch.sum(mask)\n",
    "    \n",
    "    def cross_entropy(self, outputs, labels, mask):\n",
    "        labels = labels.clone()\n",
    "        labels[mask == 0] = -999\n",
    "        \n",
    "        return nn.CrossEntropyLoss(ignore_index=-999)(outputs, labels.long())\n",
    "        \n",
    "    def ss8(self, outputs, labels, mask):\n",
    "        labels = torch.argmax(labels[:, :, 7:15], dim=2)\n",
    "        outputs = outputs[0].permute(0, 2, 1)\n",
    "        \n",
    "        return self.cross_entropy(outputs, labels, mask)\n",
    "        \n",
    "    def ss3(self, outputs, labels, mask):\n",
    "        structure_mask = torch.tensor([0,0,0,1,1,2,2,2]).to(device)\n",
    "\n",
    "        labels = torch.max(labels[:, :, 7:15] * structure_mask, dim=2)[0].long()\n",
    "        outputs = outputs[1].permute(0, 2, 1)\n",
    "        \n",
    "        return self.cross_entropy(outputs, labels, mask)\n",
    "        \n",
    "    def disorder(self, outputs, labels, mask):\n",
    "        # apply the disorder loss\n",
    "        labels = labels[:, :, 1].unsqueeze(2)\n",
    "        labels = torch.argmax(torch.cat([labels, 1-labels], dim=2), dim=2)\n",
    "        \n",
    "        outputs = outputs[2].permute(0, 2, 1)\n",
    "        \n",
    "        return self.cross_entropy(outputs, labels, mask)\n",
    "        \n",
    "    def rsa(self, outputs, labels, mask):\n",
    "        labels = labels[:, :, 5].unsqueeze(2)\n",
    "        outputs = outputs[3]\n",
    "        \n",
    "        mask = mask.unsqueeze(2)\n",
    "        \n",
    "        return self.mse(outputs, labels, mask)\n",
    "        \n",
    "    def phi(self, outputs, labels, mask):\n",
    "        labels = labels[:, :, 15].unsqueeze(2)\n",
    "        outputs = outputs[4]\n",
    "        \n",
    "        mask = mask * (labels != 360).squeeze(2).int()\n",
    "        mask = torch.cat(2*[mask.unsqueeze(2)], dim=2)\n",
    "        \n",
    "        loss = self.mse(outputs.squeeze(2), torch.cat((torch.sin(dihedral_to_radians(labels)), torch.cos(dihedral_to_radians(labels))), dim=2).squeeze(2), mask)\n",
    "        return loss\n",
    "        \n",
    "    def psi(self, outputs, labels, mask):\n",
    "        labels = labels[:, :, 16].unsqueeze(2)\n",
    "        outputs = outputs[5]\n",
    "        \n",
    "        mask = mask * (labels != 360).squeeze(2).int()\n",
    "        mask = torch.cat(2*[mask.unsqueeze(2)], dim=2)\n",
    "        \n",
    "        loss = self.mse(outputs.squeeze(2), torch.cat((torch.sin(dihedral_to_radians(labels)), torch.cos(dihedral_to_radians(labels))), dim=2).squeeze(2), mask)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, outputs, labels, weighted=True):\n",
    "        \"\"\" Forwarding of the multitaskloss input\n",
    "        Args:\n",
    "            outputs (torch.tensor): output data from model\n",
    "            labels (torch.tensor): corresponding labels for the output\n",
    "        \"\"\"\n",
    "\n",
    "        # filters\n",
    "        zero_mask = labels[:, :, 0]\n",
    "        disorder_mask = labels[:, :, 1]\n",
    "        unknown_mask = labels[:, :, -1]\n",
    "        \n",
    "        # weighted losses\n",
    "        ss8 = self.ss8(outputs, labels, zero_mask) * 1\n",
    "        ss3 = self.ss3(outputs, labels, zero_mask) * 5\n",
    "        dis = self.disorder(outputs, labels, zero_mask) * 5\n",
    "        rsa = self.rsa(outputs, labels, zero_mask * disorder_mask * unknown_mask) * 100\n",
    "        phi = self.phi(outputs, labels, zero_mask * disorder_mask * unknown_mask) * 5\n",
    "        psi = self.psi(outputs, labels, zero_mask * disorder_mask * unknown_mask) * 5\n",
    "        \n",
    "        loss = torch.stack([ss8, ss3, dis, rsa, phi, psi])\n",
    "        \n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-sheet",
   "metadata": {
    "papermill": {
     "duration": 0.023679,
     "end_time": "2021-03-01T13:49:08.372189",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.348510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-wildlife",
   "metadata": {
    "papermill": {
     "duration": 0.024243,
     "end_time": "2021-03-01T13:49:08.419112",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.394869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model trains phi and psi into each their vector that will contain calculated cos and sin values from the angle. Pytorch works with radians, so these helper functions helps converting dihedral angle values into radians and cos and sin radian values back to a dihedral angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "activated-footwear",
   "metadata": {
    "papermill": {
     "duration": 0.03093,
     "end_time": "2021-03-01T13:49:08.472343",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.441413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dihedral_to_radians(angle):\n",
    "    \"\"\" Converts angles to radians\n",
    "    Args:\n",
    "        angles (1D Tensor): vector with angle values\n",
    "    \"\"\"\n",
    "    return angle*np.pi/180\n",
    "    \n",
    "def arctan_dihedral(sin, cos):\n",
    "    \"\"\" Converts sin and cos back to diheral angles\n",
    "    Args:\n",
    "        sin (1D Tensor): vector with sin values \n",
    "        cos (1D Tensor): vector with cos values\n",
    "    \"\"\"\n",
    "    result = torch.where(cos >= 0, torch.arctan(sin/cos), torch.arctan(sin/cos)+np.pi)\n",
    "    result = torch.where((sin <= 0) & (cos <= 0), result-np.pi*2, result)\n",
    "    \n",
    "    return result*180/np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-rebound",
   "metadata": {
    "papermill": {
     "duration": 0.025486,
     "end_time": "2021-03-01T13:49:08.732735",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.707249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-theory",
   "metadata": {
    "papermill": {
     "duration": 0.022814,
     "end_time": "2021-03-01T13:49:08.778091",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.755277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Instanciating the model, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scenic-conspiracy",
   "metadata": {
    "papermill": {
     "duration": 0.029602,
     "end_time": "2021-03-01T13:49:08.830426",
     "exception": false,
     "start_time": "2021-03-01T13:49:08.800824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_model(initial_channels, hidden_neurons, cnn_output, learning_rate, params):\n",
    "    \"\"\" Initializes a model, criterion and optimizer \n",
    "    Args:\n",
    "        initial_channels (int): amount of initial inputs for the model\n",
    "        hidden_neurons (int): amount of hidden neurons in the model\n",
    "    \"\"\"\n",
    "    nsp_net = NSP_Network(initial_channels, hidden_neurons, cnn_output, params)\n",
    "    criterion = MultiTaskLoss()\n",
    "\n",
    "    # enable cuda on model and criterion if possible\n",
    "    if device.type != \"cpu\":\n",
    "        nsp_net.cuda(device)\n",
    "        criterion.cuda(device)\n",
    "\n",
    "    # optimizer for model and criterion\n",
    "    optimizer = optim.Adam([{\"params\" : criterion.parameters()}, {\"params\" : nsp_net.parameters()}], lr=learning_rate)\n",
    "    \n",
    "    return nsp_net, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-continuity",
   "metadata": {
    "papermill": {
     "duration": 0.025981,
     "end_time": "2021-03-01T13:49:12.208391",
     "exception": false,
     "start_time": "2021-03-01T13:49:12.182410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model is trained by iterating through the training dataset for an amount of epochs. The model is trained using backpropagation using the multiple task loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beautiful-constant",
   "metadata": {
    "papermill": {
     "duration": 0.035071,
     "end_time": "2021-03-01T13:49:12.267331",
     "exception": false,
     "start_time": "2021-03-01T13:49:12.232260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training(epochs, model, criterion, optimizer, dataset):\n",
    "    # iterate over the dataset multiple times\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "        \n",
    "    train, test = dataset\n",
    "\n",
    "    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, steps_per_epoch=len(train), epochs=epochs)\n",
    "\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #print('Epoch:', epoch + 1,' of ', epochs)\n",
    "\n",
    "        # training of the model \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train, 0):\n",
    "\n",
    "            # move data tensors to GPU if possible\n",
    "            inputs, labels, lengths = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            padding = labels[:, :, 0]\n",
    "            outputs = model(inputs, labels, padding)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backpropagation by custom criterion\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        training_loss.append(running_loss / len(train))\n",
    "        #print(\"Training loss: \", round(training_loss[epoch], 3))\n",
    "\n",
    "        #validation of the model\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test, 0):\n",
    "                \n",
    "                # move data tensors to GPU if possible\n",
    "                inputs, labels, lengths = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                padding = labels[:, :, 0]\n",
    "                outputs = model(inputs, labels, padding)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        validation_loss.append(running_loss / len(test))\n",
    "        #print(\"Validation loss: \", round(validation_loss[epoch], 3))\n",
    "\n",
    "        early_stopping(validation_loss[epoch], model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            #print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "        \n",
    "    #print('Finished Training')\n",
    "    \n",
    "    return training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-resolution",
   "metadata": {
    "papermill": {
     "duration": 0.023384,
     "end_time": "2021-03-01T13:49:12.313728",
     "exception": false,
     "start_time": "2021-03-01T13:49:12.290344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The training loss and validation loss are visualized to evaluate the training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opposite-nevada",
   "metadata": {
    "papermill": {
     "duration": 0.028477,
     "end_time": "2021-03-01T13:49:12.365299",
     "exception": false,
     "start_time": "2021-03-01T13:49:12.336822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(training_loss, validation_loss, title):\n",
    "    pylab.plot(training_loss)\n",
    "    pylab.plot(validation_loss)\n",
    "    pylab.xlabel('Epochs')\n",
    "    pylab.ylabel('Loss [sum]')\n",
    "    pylab.legend(('Training loss', 'Validation loss'))\n",
    "    pylab.title(title)\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-narrow",
   "metadata": {},
   "source": [
    "Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-15 13:11:13,457]\u001b[0m A new study created in memory with name: no-name-91f35318-b5e2-4ad0-8709-35bcfaaf0fa3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 24.998795).  Saving model ...\n",
      "Validation loss decreased (24.998795 --> 21.521193).  Saving model ...\n",
      "Validation loss decreased (21.521193 --> 19.974209).  Saving model ...\n",
      "Validation loss decreased (19.974209 --> 18.782351).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "initial_channels = 1300\n",
    "cnn_out = 32\n",
    "hidden_neurons = 57\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        #\"dropout_rate\" : trial.suggest_uniform(\"dropout_rate\", 0.4, 0.5),\n",
    "        \"num_layers_encoder\": trial.suggest_int(\"num_layers_encoder\", 1, 6),\n",
    "        \"learning_rate\": trial.suggest_loguniform('learning_rate', 1e-6, 1e-1),\n",
    "    }\n",
    "    \n",
    "    hyperoptim = init_model(initial_channels, hidden_neurons, cnn_out, params['learning_rate'], params)\n",
    "    \n",
    "    training_loss, evaluation_loss = training(epochs, hyperoptim[0], hyperoptim[1], hyperoptim[2], train_hhblits)\n",
    "    \n",
    "    return min(evaluation_loss)\n",
    "    \n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best trial\")\n",
    "best_parameters = study.best_trial\n",
    "\n",
    "print(best_parameters.values)\n",
    "print(best_parameters.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-press",
   "metadata": {
    "papermill": {
     "duration": 2082.879782,
     "end_time": "2021-03-01T14:23:55.267408",
     "exception": false,
     "start_time": "2021-03-01T13:49:12.387626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "initial_channels = 1300\n",
    "cnn_out = 32\n",
    "hidden_neurons = 57\n",
    "learning_rate = 1e-3\n",
    "\n",
    "#Trial 19 finished with value: 17.18759536743164 and parameters: {'dropout_rate': 0.10882566406017377, 'num_layers_encoder': 2, 'learning_rate': 0.0003437625388266798}. Best is trial 18 with value: 16.15894889831543.\n",
    "\n",
    "nsp_hhblits = init_model(initial_channels, hidden_neurons, cnn_out, learning_rate)\n",
    "\n",
    "training_loss, evaluation_loss = training(epochs, nsp_hhblits[0],\n",
    "                                                    nsp_hhblits[1],\n",
    "                                                    nsp_hhblits[2],\n",
    "                                                    train_hhblits)\n",
    "\n",
    "plot_loss(training_loss, evaluation_loss, \"NSP HHblits training and validation loss\")\n",
    "\n",
    "#training_loss, evaluation_loss = training(epochs, nsp_mmseqs[0], \n",
    "#                                          nsp_mmseqs[1], \n",
    "#                                          nsp_mmseqs[2],\n",
    "#                                          train_mmseqs)\n",
    "\n",
    "#plot_loss(training_loss, evaluation_loss, \"NSP MMseqs training and validation loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-series",
   "metadata": {
    "papermill": {
     "duration": 0.037133,
     "end_time": "2021-03-01T14:23:55.343281",
     "exception": false,
     "start_time": "2021-03-01T14:23:55.306148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-break",
   "metadata": {
    "papermill": {
     "duration": 0.03606,
     "end_time": "2021-03-01T14:23:55.415483",
     "exception": false,
     "start_time": "2021-03-01T14:23:55.379423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Evaluation functions are created to evaluate the models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-grenada",
   "metadata": {
    "papermill": {
     "duration": 0.049743,
     "end_time": "2021-03-01T14:23:55.503881",
     "exception": false,
     "start_time": "2021-03-01T14:23:55.454138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fpr(pred, labels):\n",
    "    \"\"\" False positive rate\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted binary numeric values\n",
    "        labels (1D Tensor): vector with correct binary numeric values\n",
    "    \"\"\"\n",
    "    fp = sum((pred == 1) & (labels == 0))\n",
    "    tn = sum((pred == 0) & (labels == 0))\n",
    "    \n",
    "    return (fp/(fp+tn)).item()\n",
    "\n",
    "def fnr(pred, labels):\n",
    "    \"\"\" False negative rate\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted binary numeric values\n",
    "        labels (1D Tensor): vector with correct binary numeric values\n",
    "    \"\"\"\n",
    "    fn = sum((pred == 0) & (labels == 1))\n",
    "    tp = sum((pred == 1) & (labels == 1))\n",
    "    \n",
    "    return (fn/(fn+tp)).item()\n",
    "\n",
    "def mcc(pred, labels):\n",
    "    \"\"\" Mathews correlation coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted binary numeric values\n",
    "        labels (1D Tensor): vector with correct binary numeric values\n",
    "    \"\"\"\n",
    "    fp = sum((pred == 1) & (labels == 0))\n",
    "    tp = sum((pred == 1) & (labels == 1))\n",
    "    fn = sum((pred == 0) & (labels == 1))\n",
    "    tn = sum((pred == 0) & (labels == 0))\n",
    "    \n",
    "    return ((tp*tn-fp*fn)/torch.sqrt(((tp+fp)*(fn+tn)*(tp+fn)*(fp+tn)).float())).item()\n",
    "\n",
    "def pcc(pred, labels):\n",
    "    \"\"\" Pearson correlation coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted numeric values\n",
    "        labels (1D Tensor): vector with correct numeric values\n",
    "    \"\"\"\n",
    "    x = pred - torch.mean(pred)\n",
    "    y = labels - torch.mean(labels)\n",
    "    \n",
    "    return (torch.sum(x * y) / (torch.sqrt(torch.sum(x ** 2)) * torch.sqrt(torch.sum(y ** 2)))).item()\n",
    "\n",
    "def mae(pred, labels):\n",
    "    \"\"\" Mean absolute error\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted numeric values\n",
    "        labels (1D Tensor): vector with correct numeric values\n",
    "    \"\"\"\n",
    "    err = torch.abs(labels - pred)\n",
    "    return torch.mean(torch.fmin(err, 360-err)).item()\n",
    "\n",
    "def accuracy(pred, labels):\n",
    "    \"\"\" Accuracy coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted integer values\n",
    "        labels (1D Tensor): vector with correct integer values\n",
    "    \"\"\"\n",
    "    \n",
    "    return (sum((pred == labels)) / len(labels)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-sapphire",
   "metadata": {
    "papermill": {
     "duration": 0.035867,
     "end_time": "2021-03-01T14:23:55.576571",
     "exception": false,
     "start_time": "2021-03-01T14:23:55.540704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Seperate datasets are used to evaluate the model. Here is each class evaluated and the prediction benchmarks for the trained model are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-booth",
   "metadata": {
    "papermill": {
     "duration": 0.046254,
     "end_time": "2021-03-01T14:23:55.658274",
     "exception": false,
     "start_time": "2021-03-01T14:23:55.612020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_ss8(outputs, labels, mask):\n",
    "    labels = torch.argmax(labels[:, :, 7:15], dim=2)[mask == 1]\n",
    "    outputs = torch.argmax(outputs, dim=2)[mask == 1]\n",
    "        \n",
    "    return accuracy(outputs, labels)\n",
    "    \n",
    "def evaluate_ss3(outputs, labels, mask):\n",
    "    structure_mask = torch.tensor([0,0,0,1,1,2,2,2])\n",
    "\n",
    "    labels = torch.max(labels[:, :, 7:15] * structure_mask, dim=2)[0].long()[mask == 1]\n",
    "    outputs = torch.argmax(outputs, dim=2)[mask == 1]\n",
    "        \n",
    "    return accuracy(outputs, labels)\n",
    "    \n",
    "def evaluate_dis(outputs, labels, mask, metric=\"fpr\"):\n",
    "    labels = labels[:, :, 1].unsqueeze(2)\n",
    "    labels = torch.argmax(torch.cat([labels, 1-labels], dim=2), dim=2)[mask == 1]\n",
    "    outputs = torch.argmax(outputs, dim=2)[mask == 1]\n",
    "\n",
    "    if metric == \"fpr\":\n",
    "        return fpr(outputs, labels)\n",
    "    else:\n",
    "        return mcc(outputs, labels)\n",
    "    \n",
    "def evaluate_rsa(outputs, labels, mask):\n",
    "    labels = labels[:, :, 5].unsqueeze(2)[mask == 1]\n",
    "    outputs = outputs[mask == 1]\n",
    "        \n",
    "    return pcc(outputs, labels)\n",
    "\n",
    "def evaluate_asa(outputs, labels, mask):\n",
    "    outputs = (outputs * labels[:, :, 17].unsqueeze(2))[mask == 1]\n",
    "    labels = labels[:, :, 3].unsqueeze(2)[mask == 1]\n",
    "    \n",
    "    return pcc(outputs, labels)\n",
    "\n",
    "def evaluate_phi(outputs, labels, mask):    \n",
    "    labels = labels[:, :, 15]\n",
    "    \n",
    "    mask = mask * (labels != 360).int()\n",
    "    labels = labels[mask == 1]\n",
    "    \n",
    "    outputs = arctan_dihedral(outputs[:, :, 0], outputs[:, :, 1])[mask == 1]\n",
    "    \n",
    "    return mae(outputs, labels)\n",
    "    \n",
    "def evaluate_psi(outputs, labels, mask):\n",
    "    labels = labels[:, :, 16]\n",
    "    \n",
    "    mask = mask * (labels != 360).int()\n",
    "    labels = labels[mask == 1]\n",
    "    \n",
    "    outputs = arctan_dihedral(outputs[:, :, 0], outputs[:, :, 1])[mask == 1]\n",
    "    \n",
    "    return mae(outputs, labels)\n",
    "\n",
    "def evaluation(model, dataset):    \n",
    "    # iterate through the evaluation dataset\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            # move data tensors to GPU if possible\n",
    "            inputs, labels, lengths = data\n",
    "            \n",
    "            # add evaluation mask to the masks\n",
    "            evaluation_mask = labels[:, :, 2]\n",
    "            \n",
    "            zero_mask = labels[:, :, 0] * evaluation_mask\n",
    "            disorder_mask = labels[:, :, 1]\n",
    "            unknown_mask = labels[:, :, -1]\n",
    "            \n",
    "            # get predictions\n",
    "            model = model.to('cpu')\n",
    "            predictions = model(inputs, lengths, zero_mask) # predict values\n",
    "            \n",
    "            # move predictions to cpu\n",
    "            for i in range(len(predictions)):\n",
    "                predictions[i] = predictions[i].cpu()\n",
    "                \n",
    "            labels = labels.cpu()\n",
    "            zero_mask = zero_mask.cpu()\n",
    "            disorder_mask = disorder_mask.cpu()\n",
    "            unknown_mask = unknown_mask.cpu()\n",
    "            \n",
    "            # evaluate\n",
    "            ss8 = evaluate_ss8(predictions[0], labels, zero_mask)\n",
    "            ss3 = evaluate_ss3(predictions[1], labels, zero_mask)\n",
    "            dis_fpr = evaluate_dis(predictions[2], labels, zero_mask, metric=\"fpr\")\n",
    "            dis_mcc = evaluate_dis(predictions[2], labels, zero_mask, metric=\"mcc\")\n",
    "            rsa = evaluate_rsa(predictions[3], labels, zero_mask * disorder_mask * unknown_mask)\n",
    "            asa = evaluate_asa(predictions[3], labels, zero_mask * disorder_mask * unknown_mask)\n",
    "            phi = evaluate_phi(predictions[4], labels, zero_mask * disorder_mask * unknown_mask)\n",
    "            psi = evaluate_psi(predictions[5], labels, zero_mask * disorder_mask * unknown_mask)\n",
    "            \n",
    "            print(\"RSA (PCC): {}\".format(round(rsa, 3)))\n",
    "            print(\"ASA (PCC): {}\".format(round(asa, 3)))\n",
    "            print(\"SS8 [Q8]: {}\".format(round(ss8, 3)))\n",
    "            print(\"SS3 [Q3]: {}\".format(round(ss3, 3)))\n",
    "            print(\"Disorder (MCC): {}\".format(round(dis_mcc, 2)))\n",
    "            print(\"Disorder (FNR): {}\".format(round(dis_fpr, 3)))\n",
    "            print(\"Phi (MAE): {}\".format(round(phi, 3)))\n",
    "            print(\"Psi (MAE): {}\".format(round(psi, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-basket",
   "metadata": {
    "papermill": {
     "duration": 0.036489,
     "end_time": "2021-03-01T14:23:55.730909",
     "exception": false,
     "start_time": "2021-03-01T14:23:55.694420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The models are evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-ocean",
   "metadata": {
    "papermill": {
     "duration": 0.188387,
     "end_time": "2021-03-01T14:23:55.955121",
     "exception": false,
     "start_time": "2021-03-01T14:23:55.766734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## print(\"Evaluation HHblits...\")\n",
    "\n",
    "#print(\"\\nCB513\")\n",
    "#evaluation(nsp_hhblits[0], CB513_hhblits)\n",
    "#print(\"\\nTS115\")\n",
    "#evaluation(nsp_hhblits[0], TS115_hhblits)\n",
    "print(\"\\nCASP12\")\n",
    "evaluation(nsp_hhblits[0], CASP12_hhblits)\n",
    "\n",
    "#print(\"\\nEvaluation MMseqs...\")\n",
    "\n",
    "#print(\"\\nCB513\")\n",
    "#evaluation(nsp_mmseqs[0], CB513_hhblits)\n",
    "#print(\"\\nTS115\")\n",
    "#evaluation(nsp_mmseqs[0], TS115_hhblits)\n",
    "#print(\"\\nCASP12\")\n",
    "#evaluation(nsp_mmseqs[0], CASP12_hhblits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-wages",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-japan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2433.856918,
   "end_time": "2021-03-01T14:23:58.366762",
   "environment_variables": {},
   "exception": null,
   "input_path": "nsp2_rsa.ipynb",
   "output_path": "nsp2_rsa.ipynb",
   "parameters": {},
   "start_time": "2021-03-01T13:43:24.509844",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
