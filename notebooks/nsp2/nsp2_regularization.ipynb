{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetSurfP 2.0 regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to test the following regularization techniques: L1/L2, dropout, batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different libraries are initialized and pytorch is either configured to use the CPU or an available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# debugging\n",
    "import pdb\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is compressed as a numpy zip and the dimensions of the dataset are as following: \n",
    "\n",
    "**[sequence, position, label]**\n",
    "\n",
    "The labels are as following in the data (shown by index):\n",
    "- [0:20] Amino Acids (sparse encoding) (Unknown residues are stored as an all-zero vector)\n",
    "- [20:50] hmm profile\n",
    "- [50] Seq mask (1 = seq, 0 = empty)\n",
    "- [51] Disordered mask (0 = disordered, 1 = ordered)\n",
    "- [52] Evaluation mask (For CB513 dataset, 1 = eval, 0 = ignore)\n",
    "- [53] ASA (isolated)\n",
    "- [54] ASA (complexed)\n",
    "- [55] RSA (isolated)\n",
    "- [56] RSA (complexed)\n",
    "- [57:65] Q8 GHIBESTC (Q8 -> Q3: HHHEECCC)\n",
    "- [65:67] Phi+Psi\n",
    "- [67] ASA_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hhblits = np.load(\"../data/nsp2/training_data/Train_HHblits.npz\")\n",
    "\n",
    "CB513_hhblits = np.load(\"../data/nsp2/training_data/CB513_HHblits.npz\")\n",
    "TS115_hhblits = np.load(\"../data/nsp2/training_data/TS115_HHblits.npz\")\n",
    "CASP12_hhblits = np.load(\"../data/nsp2/training_data/CASP12_HHblits.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data loader class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data loader class is created to load the NSP data into a DataLoader object. The DataLoader is a pytorch object, thats used to feed the data as batches. Which will be usefull when training and evaluating the NSP model. The NSPData class divides the dataset into input data and target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSPData(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            X (np.array): The array that contains the training data\n",
    "            y (np.array): The array that contains the test data\n",
    "        \"\"\"\n",
    "        self.data = torch.tensor(dataset['data'][:, :, :50]).float()\n",
    "        self.targets = torch.tensor(dataset['data'][:, :, 50:68]).float()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns train and test data at an index\n",
    "        Args:\n",
    "            index (int): Index at the array\n",
    "        \"\"\"\n",
    "        return self.data[index], self.targets[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the data\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have a training and validation set when training to evaluate the training performance. Therefore, a function that splits the dataset into training and evaluation is created for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(batch_size, dataset, validation_fraction = 0.20):\n",
    "    \"\"\" Splits the dataset into train and validation\n",
    "    Args:\n",
    "        batch_size (int): size of each batch\n",
    "        dataset (np.array): dataset containing training data\n",
    "        validation_fraction (float): the size of the validation set as a fraction\n",
    "    \"\"\"\n",
    "        \n",
    "    num_train = len(dataset['data'])\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(validation_fraction * num_train))\n",
    "    \n",
    "    # subset the dataset\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    training_set = DataLoader(NSPData(dataset), sampler=train_sampler, batch_size=batch_size)\n",
    "    validation_set = DataLoader(NSPData(dataset), sampler=valid_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return training_set, validation_set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instanciate data into the data loader class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training datasets are splitted into training and validation for training. Whereas the test sets are simply loaded, since they are independent datasets. The batch sizes is set to 15. The default training/validation split is 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation sets\n",
    "train_hhblits = split_dataset(batch_size, train_hhblits)\n",
    "\n",
    "# test sets\n",
    "CB513_hhblits = DataLoader(NSPData(CB513_hhblits), batch_size=batch_size)\n",
    "TS115_hhblits = DataLoader(NSPData(TS115_hhblits), batch_size=batch_size)\n",
    "CASP12_hhblits = DataLoader(NSPData(CASP12_hhblits), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The NSP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer consists of one-hot encoded sequences (20 features amino acids) and a full HMM profile (20 features for amino acid profile, 7 features of state transition probabilities and 3 features for local alignment diversity) The input layer is followed by two layers of 1D CNN layers, that consist of 32 filters with a size of 129 and 257. Whereas the output of the last 1D CNN is concatenated with the initial input features. These residuals connections are used to achieve a better backpropagation without gradient degradation and a deeper network. The concatenated input+residuals is then applied to 2 bidirectional long  short-term memory (LSTM) layers with 1024 nodes, that outputs 2048 hidden neurons. The hidden neurons output is input to a fully connected (FC) layer to predict the 18 classes (RSA, SS8, SS3, φ, ψ, and disorder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSP_Network(nn.Module):\n",
    "    def __init__(self, init_channels, n_hidden, dropout=False, batch_norm=False):\n",
    "        \"\"\" Constructor of the NSP network\n",
    "        Args:\n",
    "            init_channels (int): The size of the incoming feature vector\n",
    "            n_hidden: (int) The amount of hidden neurons in the bidirectional lstm\n",
    "        \"\"\"\n",
    "        super(NSP_Network, self).__init__()\n",
    "        \n",
    "        # residual blocks\n",
    "        self.conv1 = [\n",
    "            nn.Conv1d(in_channels=init_channels, out_channels=32, kernel_size=4, padding=1),\n",
    "            nn.ReLU()]\n",
    "         \n",
    "        self.conv2 = [\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=4, padding=2),\n",
    "            nn.ReLU()]\n",
    "            \n",
    "        # bidirectional lstm\n",
    "        self.lstm = [nn.LSTM(input_size=init_channels+64, hidden_size=n_hidden, num_layers=2, \\\n",
    "                        batch_first=True, bidirectional=True)]\n",
    "        \n",
    "        # add batch normalization\n",
    "        if batch_norm:\n",
    "            self.conv1.append(nn.BatchNorm1d(32))\n",
    "            self.conv2.append(nn.BatchNorm1d(64))\n",
    "\n",
    "        # add dropout\n",
    "        if dropout:\n",
    "            self.conv1.append(nn.Dropout(p=0.5))\n",
    "            self.conv2.append(nn.Dropout(p=0.5))\n",
    "            self.lstm = [nn.LSTM(input_size=init_channels+64, hidden_size=n_hidden, num_layers=2, \\\n",
    "                        batch_first=True, bidirectional=True, dropout=0.5)]\n",
    "            \n",
    "        # convert to sequential\n",
    "        self.conv1 = nn.Sequential(*self.conv1)\n",
    "        self.conv2 = nn.Sequential(*self.conv2)\n",
    "        self.lstm = nn.Sequential(*self.lstm)\n",
    "        \n",
    "        # output tasks\n",
    "        self.ss8 = nn.Linear(in_features=n_hidden*2, out_features=8)\n",
    "        self.ss3 = nn.Linear(in_features=n_hidden*2, out_features=3)\n",
    "        self.disorder = nn.Linear(in_features=n_hidden*2, out_features=2)\n",
    "        self.rsa = nn.Linear(in_features=n_hidden*2, out_features=1)\n",
    "        self.phi = nn.Linear(in_features=n_hidden*2, out_features=2)\n",
    "        self.psi = nn.Linear(in_features=n_hidden*2, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forwarding of the classifier input\n",
    "        Args:\n",
    "            x (torch.tensor): input data\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate the residuals\n",
    "        x = x.permute(0,2,1)\n",
    "        r = self.conv1(x)\n",
    "        r = self.conv2(r)\n",
    "                \n",
    "        # concatenate channels from residuals and input\n",
    "        x = torch.cat([r, x],dim=1)\n",
    "        \n",
    "        # calculate double layer bidirectional lstm\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # hidden neurons to classes\n",
    "        ss8 = self.ss8(x)\n",
    "        ss3 = self.ss3(x)\n",
    "        disorder = self.disorder(x)\n",
    "        rsa = self.rsa(x)\n",
    "        phi = self.phi(x)\n",
    "        psi = self.psi(x)\n",
    "\n",
    "        return [ss8, ss3, disorder, rsa, phi, psi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, loss functions are required. The tasks SS8, SS3 and disorder uses cross entropy loss function and RSA, φ and ψ use mean squared error loss.\n",
    "\n",
    "Weights were adjusted so each loss contribution was approximately equal by using homoscedastic uncertainty and optimizing the log variances for each prediction. (https://arxiv.org/abs/1705.07115)\n",
    "\n",
    "In the forwarding of the MultiTaskLoss, filters are used to remove extrapolated zeros or disordered regions from the calculated losses.\n",
    "\n",
    "The indexes for the labels are described in the data preparation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\" Weighs multiple loss functions by considering the \n",
    "        homoscedastic uncertainty of each task \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        \n",
    "        self.log_vars = nn.Parameter(torch.zeros(6))\n",
    "\n",
    "    def forward(self, outputs, labels, weight_scale = True):\n",
    "        \"\"\" Forwarding of the multitaskloss input\n",
    "        Args:\n",
    "            outputs (torch.tensor): output data from model\n",
    "            labels (torch.tensor): corresponding labels for the output\n",
    "        \"\"\"\n",
    "        \n",
    "        # filters\n",
    "        zero_mask = (labels[:, :, 0] == 1)\n",
    "        disorder_mask = (labels[:, :, 1] == 1)\n",
    "        \n",
    "        # apply the Q8 loss\n",
    "        SS8_labels = torch.argmax(labels[:, :, 7:15], dim=2)\n",
    "        SS8_inputs = outputs[0].permute(0, 2, 1)\n",
    "        SS8_loss = nn.CrossEntropyLoss(reduction=\"none\")(SS8_inputs, SS8_labels)\n",
    "        \n",
    "        SS8_loss = SS8_loss[zero_mask | disorder_mask].sum()\n",
    "        \n",
    "        # apply the SS3 loss\n",
    "        SS3_mask = torch.tensor([0,0,0,1,1,2,2,2], device=device)\n",
    "\n",
    "        SS3_labels = torch.max(labels[:, :, 7:15] * SS3_mask, dim=2)[0].long()\n",
    "        SS3_inputs = outputs[1].permute(0, 2, 1)\n",
    "        SS3_loss = nn.CrossEntropyLoss(reduction=\"none\")(SS3_inputs, SS3_labels)\n",
    "        \n",
    "        SS3_loss = SS3_loss[zero_mask | disorder_mask].sum()\n",
    "\n",
    "        # apply the disorder loss\n",
    "        disorder_labels = labels[:, :, 1].long()\n",
    "        disorder_inputs = outputs[2].permute(0, 2, 1)\n",
    "        disorder_loss = nn.CrossEntropyLoss(reduction=\"none\")(disorder_inputs, disorder_labels)\n",
    "        \n",
    "        disorder_loss = disorder_loss[zero_mask].sum()\n",
    "\n",
    "        # apply RSA loss\n",
    "        rsa_labels = labels[:, :, 6]\n",
    "        rsa_inputs = outputs[3].squeeze(2)\n",
    "        rsa_loss = nn.MSELoss(reduction=\"none\")(rsa_inputs, rsa_labels)\n",
    "        \n",
    "        rsa_loss = rsa_loss[zero_mask].sum()\n",
    "\n",
    "        # apply phi loss\n",
    "        phi_labels = labels[:, :, 15].unsqueeze(2)\n",
    "        phi_inputs = outputs[4]\n",
    "         \n",
    "        phi_loss = nn.MSELoss(reduction=\"none\")(phi_inputs, \\\n",
    "                        torch.cat((torch.sin(dihedral_to_radians(phi_labels)), torch.cos(dihedral_to_radians(phi_labels))), dim=2))\n",
    "        \n",
    "        phi_loss = phi_loss[zero_mask].sum()\n",
    "        \n",
    "        # apply psi loss\n",
    "        psi_labels = labels[:, :, 16].unsqueeze(2)\n",
    "        psi_inputs = outputs[5]\n",
    "        \n",
    "        psi_loss = nn.MSELoss(reduction=\"none\")(psi_inputs, \\\n",
    "                        torch.cat((torch.sin(dihedral_to_radians(psi_labels)), torch.cos(dihedral_to_radians(psi_labels))), dim=2))\n",
    "        \n",
    "        psi_loss = psi_loss[zero_mask].sum()\n",
    "        \n",
    "        # weighted losses\n",
    "        loss = torch.stack([SS8_loss, SS3_loss, disorder_loss, rsa_loss, phi_loss, psi_loss])\n",
    "        \n",
    "        if weight_scale:\n",
    "            loss = torch.exp(-self.log_vars) * loss + self.log_vars\n",
    "        \n",
    "        return loss.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trains phi and psi into each their vector that will contain calculated cos and sin values from the angle. Pytorch works with radians, so these helper functions helps converting dihedral angle values into radians and cos and sin radian values back to a dihedral angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dihedral_to_radians(angle):\n",
    "    \"\"\" Converts angles to radians\n",
    "    Args:\n",
    "        angles (1D Tensor): vector with angle values\n",
    "    \"\"\"\n",
    "    return angle*np.pi/180\n",
    "    \n",
    "def arctan_dihedral(sin, cos):\n",
    "    \"\"\" Converts sin and cos back to diheral angles\n",
    "    Args:\n",
    "        sin (1D Tensor): vector with sin values \n",
    "        cos (1D Tensor): vector with cos values\n",
    "    \"\"\"\n",
    "    result = torch.where(cos >= 0, torch.arctan(sin/cos), torch.arctan(sin/cos)+np.pi)\n",
    "    result = torch.where((sin <= 0) & (cos <= 0), result-np.pi*2, result)\n",
    "    \n",
    "    return result*180/np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciating the model, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(initial_channels, hidden_neurons, lr=1e-3, dropout=False, batch_norm=False, weight_decay=0):\n",
    "    \"\"\" Initializes a model, criterion and optimizer \n",
    "    Args:\n",
    "        initial_channels (int): amount of initial inputs for the model\n",
    "        hidden_neurons (int): amount of hidden neurons in the model\n",
    "    \"\"\"\n",
    "    \n",
    "    nsp_net = NSP_Network(initial_channels, hidden_neurons, dropout, batch_norm)\n",
    "    criterion = MultiTaskLoss()\n",
    "\n",
    "    # enable cuda on model and criterion if possible\n",
    "    if device.type != \"cpu\":\n",
    "        nsp_net.cuda(device)\n",
    "        criterion.cuda(device)\n",
    "\n",
    "    # optimizer for model and criterion\n",
    "    optimizer = optim.Adam([{\"params\" : nsp_net.parameters()},\n",
    "                            {\"params\" : criterion.parameters()}], lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    return nsp_net, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_channels = 50\n",
    "hidden_neurons = 256 #1024\n",
    "\n",
    "nsp = init_model(initial_channels, hidden_neurons)\n",
    "nsp_dropout = init_model(initial_channels, hidden_neurons, dropout=True)\n",
    "nsp_batch_norm = init_model(initial_channels, hidden_neurons, batch_norm=True)\n",
    "nsp_l2 = init_model(initial_channels, hidden_neurons, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained by iterating through the training dataset for an amount of epochs. The model is trained using backpropagation using the multiple task loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs, model, criterion, optimizer, dataset):\n",
    "    # iterate over the dataset multiple times\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    CB513_loss = []\n",
    "    TS115_loss = []\n",
    "    CASP12_loss = []\n",
    "\n",
    "    train, test = dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch:', epoch + 1,' of ', epochs)\n",
    "\n",
    "        # training of the model \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train, 0):\n",
    "            inputs, labels = data\n",
    "            # move data tensors to GPU if possible\n",
    "            inputs, labels = inputs.to(device), labels.to(device)                            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # backpropagation by custom criterion\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        training_loss.append(running_loss / len(train))\n",
    "        print(\"Training loss: \", round(training_loss[epoch], 2))\n",
    "        \n",
    "        # validation of the model\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test, 0):\n",
    "                inputs, labels = data\n",
    "                # move data tensors to GPU if possible\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels, weight_scale=False)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        validation_loss.append(running_loss / len(test))\n",
    "        print(\"Validation loss: \", round(validation_loss[epoch], 2))\n",
    "        \n",
    "        # CB513 loss\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(CB513_hhblits, 0):\n",
    "                inputs, labels = data\n",
    "                # move data tensors to GPU if possible\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels, weight_scale=False)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        CB513_loss.append(running_loss / len(CB513_hhblits))\n",
    "        print(\"CB513 loss: \", round(CB513_loss[epoch], 2))\n",
    "        \n",
    "        # TS115 loss\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(TS115_hhblits, 0):\n",
    "                inputs, labels = data\n",
    "                # move data tensors to GPU if possible\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels,  weight_scale=False)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        TS115_loss.append(running_loss / len(TS115_hhblits))\n",
    "        print(\"TS115 loss: \", round(TS115_loss[epoch], 2))\n",
    "        \n",
    "        # TS115 loss\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(CASP12_hhblits, 0):\n",
    "                inputs, labels = data\n",
    "                # move data tensors to GPU if possible\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels,  weight_scale=False)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        CASP12_loss.append(running_loss / len(CASP12_hhblits))\n",
    "        print(\"CASP12 loss: \", round(CASP12_loss[epoch], 2))\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    return [training_loss, validation_loss, CB513_loss, TS115_loss, CASP12_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss and validation loss are visualized to evaluate the training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, title):\n",
    "    for points in history:\n",
    "        pylab.plot(points)\n",
    "        \n",
    "    pylab.xlabel('Epochs')\n",
    "    pylab.ylabel('Loss [sum]')\n",
    "    pylab.legend(('Training loss', 'Validation loss', 'CB513 loss', 'TS115 loss', 'CASP12 loss'))\n",
    "    pylab.title(title)\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  of  40\n",
      "Training loss:  1535.61\n",
      "Validation loss:  1408.62\n",
      "CB513 loss:  1691.87\n",
      "TS115 loss:  1446.05\n",
      "CASP12 loss:  1669.92\n",
      "Epoch: 2  of  40\n",
      "Training loss:  821.86\n",
      "Validation loss:  1320.52\n",
      "CB513 loss:  1590.83\n",
      "TS115 loss:  1354.74\n",
      "CASP12 loss:  1574.46\n",
      "Epoch: 3  of  40\n",
      "Training loss:  558.81\n",
      "Validation loss:  1287.43\n",
      "CB513 loss:  1556.72\n",
      "TS115 loss:  1326.05\n",
      "CASP12 loss:  1529.98\n",
      "Epoch: 4  of  40\n",
      "Training loss:  397.27\n",
      "Validation loss:  1245.51\n",
      "CB513 loss:  1497.79\n",
      "TS115 loss:  1295.94\n",
      "CASP12 loss:  1543.65\n",
      "Epoch: 5  of  40\n",
      "Training loss:  293.32\n",
      "Validation loss:  1230.26\n",
      "CB513 loss:  1484.68\n",
      "TS115 loss:  1276.38\n",
      "CASP12 loss:  1475.89\n",
      "Epoch: 6  of  40\n",
      "Training loss:  220.56\n",
      "Validation loss:  1219.72\n",
      "CB513 loss:  1472.02\n",
      "TS115 loss:  1265.89\n",
      "CASP12 loss:  1488.19\n",
      "Epoch: 7  of  40\n",
      "Training loss:  167.97\n",
      "Validation loss:  1196.97\n",
      "CB513 loss:  1452.63\n",
      "TS115 loss:  1239.16\n",
      "CASP12 loss:  1453.14\n",
      "Epoch: 8  of  40\n",
      "Training loss:  129.81\n",
      "Validation loss:  1186.32\n",
      "CB513 loss:  1439.4\n",
      "TS115 loss:  1230.54\n",
      "CASP12 loss:  1453.08\n",
      "Epoch: 9  of  40\n",
      "Training loss:  101.48\n",
      "Validation loss:  1185.81\n",
      "CB513 loss:  1437.64\n",
      "TS115 loss:  1230.2\n",
      "CASP12 loss:  1478.29\n",
      "Epoch: 10  of  40\n",
      "Training loss:  79.66\n",
      "Validation loss:  1178.83\n",
      "CB513 loss:  1433.43\n",
      "TS115 loss:  1217.58\n",
      "CASP12 loss:  1430.81\n",
      "Epoch: 11  of  40\n",
      "Training loss:  62.75\n",
      "Validation loss:  1172.54\n",
      "CB513 loss:  1426.78\n",
      "TS115 loss:  1217.12\n",
      "CASP12 loss:  1441.04\n",
      "Epoch: 12  of  40\n",
      "Training loss:  49.87\n",
      "Validation loss:  1166.07\n",
      "CB513 loss:  1415.52\n",
      "TS115 loss:  1216.66\n",
      "CASP12 loss:  1427.81\n",
      "Epoch: 13  of  40\n",
      "Training loss:  40.1\n",
      "Validation loss:  1167.05\n",
      "CB513 loss:  1419.5\n",
      "TS115 loss:  1228.23\n",
      "CASP12 loss:  1439.92\n",
      "Epoch: 14  of  40\n",
      "Training loss:  32.34\n",
      "Validation loss:  1166.29\n",
      "CB513 loss:  1414.62\n",
      "TS115 loss:  1223.54\n",
      "CASP12 loss:  1423.43\n",
      "Epoch: 15  of  40\n",
      "Training loss:  26.35\n",
      "Validation loss:  1178.45\n",
      "CB513 loss:  1432.96\n",
      "TS115 loss:  1246.22\n",
      "CASP12 loss:  1478.89\n",
      "Epoch: 16  of  40\n",
      "Training loss:  21.8\n",
      "Validation loss:  1184.76\n",
      "CB513 loss:  1440.32\n",
      "TS115 loss:  1228.46\n",
      "CASP12 loss:  1479.97\n",
      "Epoch: 17  of  40\n",
      "Training loss:  18.27\n",
      "Validation loss:  1184.12\n",
      "CB513 loss:  1441.36\n",
      "TS115 loss:  1230.21\n",
      "CASP12 loss:  1465.33\n",
      "Epoch: 18  of  40\n",
      "Training loss:  15.46\n",
      "Validation loss:  1180.71\n",
      "CB513 loss:  1428.99\n",
      "TS115 loss:  1221.01\n",
      "CASP12 loss:  1475.31\n",
      "Epoch: 19  of  40\n",
      "Training loss:  13.41\n",
      "Validation loss:  1197.27\n",
      "CB513 loss:  1455.13\n",
      "TS115 loss:  1241.12\n",
      "CASP12 loss:  1464.22\n",
      "Epoch: 20  of  40\n",
      "Training loss:  11.77\n",
      "Validation loss:  1211.22\n",
      "CB513 loss:  1469.25\n",
      "TS115 loss:  1269.99\n",
      "CASP12 loss:  1543.47\n",
      "Epoch: 21  of  40\n",
      "Training loss:  10.6\n",
      "Validation loss:  1212.13\n",
      "CB513 loss:  1461.35\n",
      "TS115 loss:  1269.85\n",
      "CASP12 loss:  1531.54\n",
      "Epoch: 22  of  40\n",
      "Training loss:  9.64\n",
      "Validation loss:  1222.21\n",
      "CB513 loss:  1478.83\n",
      "TS115 loss:  1279.36\n",
      "CASP12 loss:  1558.82\n",
      "Epoch: 23  of  40\n",
      "Training loss:  8.96\n",
      "Validation loss:  1243.72\n",
      "CB513 loss:  1501.45\n",
      "TS115 loss:  1292.88\n",
      "CASP12 loss:  1567.12\n",
      "Epoch: 24  of  40\n"
     ]
    }
   ],
   "source": [
    "epochs = 40 #50\n",
    "\n",
    "history = training(epochs, nsp[0], nsp[1], nsp[2], train_hhblits)\n",
    "plot_loss(history, \"NSP HHblits training, validation and test loss\")\n",
    "\n",
    "history = training(epochs, nsp_dropout[0], nsp_dropout[1], nsp_dropout[2], train_hhblits)\n",
    "plot_loss(history, \"NSP HHblits training, validation and test loss with dropout p=0.5\")\n",
    "\n",
    "history = training(epochs, nsp_batch_norm[0], nsp_batch_norm[1], nsp_batch_norm[2], train_hhblits)\n",
    "plot_loss(history, \"NSP HHblits training, validation and test loss with batch normalization\")\n",
    "\n",
    "history = training(epochs, nsp_l2[0], nsp_l2[1], nsp_l2[2], train_hhblits)\n",
    "plot_loss(history, \"NSP HHblits training, validation and test loss with L2 regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation functions are created to evaluate the models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(pred, labels):\n",
    "    \"\"\" False positive rate\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted binary numeric values\n",
    "        labels (1D Tensor): vector with correct binary numeric values\n",
    "    \"\"\"\n",
    "    fp = sum((pred == 1) & (labels == 0))\n",
    "    tn = sum((pred == 0) & (labels == 0))\n",
    "    \n",
    "    return (fp/(fp+tn)).item()\n",
    "\n",
    "def fnr(pred, labels):\n",
    "    \"\"\" False negative rate\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted binary numeric values\n",
    "        labels (1D Tensor): vector with correct binary numeric values\n",
    "    \"\"\"\n",
    "    fn = sum((pred == 0) & (labels == 1))\n",
    "    tp = sum((pred == 1) & (labels == 1))\n",
    "    \n",
    "    return (fn/(fn+tp)).item()\n",
    "\n",
    "def mcc(pred, labels):\n",
    "    \"\"\" Mathews correlation coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted binary numeric values\n",
    "        labels (1D Tensor): vector with correct binary numeric values\n",
    "    \"\"\"\n",
    "    fp = sum((pred == 1) & (labels == 0))\n",
    "    tp = sum((pred == 1) & (labels == 1))\n",
    "    fn = sum((pred == 0) & (labels == 1))\n",
    "    tn = sum((pred == 0) & (labels == 0))\n",
    "    \n",
    "    return ((tp*tn-fp*fn)/torch.sqrt(((tp+fp)*(fn+tn)*(tp+fn)*(fp+tn)).float())).item()\n",
    "\n",
    "def pcc(pred, labels):\n",
    "    \"\"\" Pearson correlation coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted numeric values\n",
    "        labels (1D Tensor): vector with correct numeric values\n",
    "    \"\"\"\n",
    "    x = pred - torch.mean(pred)\n",
    "    y = labels - torch.mean(labels)\n",
    "    \n",
    "    return (torch.sum(x * y) / (torch.sqrt(torch.sum(x ** 2)) * torch.sqrt(torch.sum(y ** 2)))).item()\n",
    "\n",
    "def mae(pred, labels):\n",
    "    \"\"\" Mean absolute error\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted numeric values\n",
    "        labels (1D Tensor): vector with correct numeric values\n",
    "    \"\"\"\n",
    "    \n",
    "    return (sum(torch.min(torch.cat((abs(labels-pred).unsqueeze(1), abs(360-labels+pred).unsqueeze(1)), dim=1), dim=1)[0])/len(labels)).item()\n",
    "    \n",
    "def accuracy(pred, labels):\n",
    "    \"\"\" Accuracy coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted integer values\n",
    "        labels (1D Tensor): vector with correct integer values\n",
    "    \"\"\"\n",
    "    \n",
    "    return (sum((pred == labels)) / len(labels)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate datasets are used to evaluate the model. Here is each class evaluated and the prediction benchmarks for the trained model are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataset):\n",
    "    # store evaluation results\n",
    "    results = {\n",
    "        \"SS8\": 0,\n",
    "        \"SS3\": 0,\n",
    "        \"disorder_mcc\": 0,\n",
    "        \"disorder_fnr\": 0,\n",
    "        \"RSA\": 0,\n",
    "        \"ASA\": 0,\n",
    "        \"psi\": 0,\n",
    "        \"phi\": 0,\n",
    "    }\n",
    "    # iterate through the evaluation dataset\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            # move data tensors to GPU if possible\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
    "\n",
    "            predictions = model(inputs) # predict values\n",
    "\n",
    "            # Use only sequences from seq mask and remove extrapolated zeroes\n",
    "            predictions = [pred[labels[:, :, 0] == 1] for pred in predictions]\n",
    "            labels = labels[labels[:, :, 0] == 1]\n",
    "\n",
    "            # calculate the SS8 accuracy\n",
    "            SS8_labels = torch.argmax(labels[:, 7:15], dim=1).flatten()\n",
    "            SS8_pred = torch.argmax(predictions[0], dim=1).flatten()\n",
    "\n",
    "            results[\"SS8\"] += accuracy(SS8_pred, SS8_labels)\n",
    "\n",
    "            # calculate the SS3 accuracy\n",
    "            SS3_mask = torch.tensor([0,0,0,1,1,2,2,2], device=device)\n",
    "\n",
    "            SS3_labels = torch.max(labels[:, 7:15] * SS3_mask, dim=1)[0].long().flatten()\n",
    "            SS3_pred = torch.argmax(predictions[1], dim=1).flatten()\n",
    "\n",
    "            results[\"SS3\"] += accuracy(SS3_pred, SS3_labels)\n",
    "\n",
    "            # calculate the FPR of disorder\n",
    "            disorder_labels = labels[:, 1].flatten()\n",
    "            disorder_pred = torch.argmax(predictions[2], dim=1).flatten()\n",
    "\n",
    "            results[\"disorder_mcc\"] += mcc(disorder_pred, disorder_labels)\n",
    "            results[\"disorder_fnr\"] += fnr(disorder_pred, disorder_labels)\n",
    "\n",
    "            # calculate the RSA\n",
    "            rsa_labels = labels[:, 6].flatten()\n",
    "            rsa_pred = predictions[3].flatten()\n",
    "\n",
    "            results['RSA'] += pcc(rsa_pred, rsa_labels)\n",
    "\n",
    "            # calculate the ASA\n",
    "            asa_labels = labels[:, 4].flatten()\n",
    "            asa_max = labels[:, 17].flatten()\n",
    "\n",
    "            results['ASA'] += pcc(asa_max * rsa_pred, asa_labels)\n",
    "\n",
    "            # calculate the psi\n",
    "            phi_labels = labels[:, 15].flatten()\n",
    "            phi_pred = arctan_dihedral(predictions[4][:, 0], predictions[4][:, 1]).flatten()\n",
    "\n",
    "            results['phi'] += mae(phi_pred, phi_labels)\n",
    "\n",
    "            # calculate the psi\n",
    "            psi_labels = labels[:, 16].flatten()\n",
    "            psi_pred = arctan_dihedral(predictions[5][:, 0], predictions[5][:, 1]).flatten()\n",
    "\n",
    "            results['psi'] += mae(psi_pred, psi_labels)\n",
    "\n",
    "        # average prediction results \n",
    "        for method, _ in results.items():\n",
    "            results[method] /= len(dataset)\n",
    "\n",
    "    print(\"RSA (PCC): {}\".format(round(results[\"RSA\"], 2)))\n",
    "    print(\"ASA (PCC): {}\".format(round(results[\"ASA\"], 2)))\n",
    "    print(\"SS8 [Q8]: {}\".format(round(results[\"SS8\"], 2)))\n",
    "    print(\"SS3 [Q3]: {}\".format(round(results[\"SS3\"], 2)))\n",
    "    print(\"Disorder (MCC): {}\".format(round(results[\"disorder_mcc\"], 2)))\n",
    "    print(\"Disorder (FNR): {}\".format(round(results[\"disorder_fnr\"], 3)))\n",
    "    print(\"Phi (MAE): {}\".format(round(results[\"phi\"], 2)))\n",
    "    print(\"Psi (MAE): {}\".format(round(results[\"psi\"], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation NSP HHblits...\")\n",
    "\n",
    "print(\"\\nCB513\")\n",
    "evaluation(nsp[0], CB513_hhblits)\n",
    "print(\"\\nTS115\")\n",
    "evaluation(nsp[0], TS115_hhblits)\n",
    "print(\"\\nCASP12\")\n",
    "evaluation(nsp[0], CASP12_hhblits)\n",
    "\n",
    "print(\"Evaluation NSP HHblits with dropout...\")\n",
    "\n",
    "print(\"\\nCB513\")\n",
    "evaluation(nsp_dropout[0], CB513_hhblits)\n",
    "print(\"\\nTS115\")\n",
    "evaluation(nsp_dropout[0], TS115_hhblits)\n",
    "print(\"\\nCASP12\")\n",
    "evaluation(nsp_dropout[0], CASP12_hhblits)\n",
    "\n",
    "\n",
    "print(\"Evaluation NSP HHblits with batch normalization...\")\n",
    "\n",
    "print(\"\\nCB513\")\n",
    "evaluation(nsp_batch_norm[0], CB513_hhblits)\n",
    "print(\"\\nTS115\")\n",
    "evaluation(nsp_batch_norm[0], TS115_hhblits)\n",
    "print(\"\\nCASP12\")\n",
    "evaluation(nsp_batch_norm[0], CASP12_hhblits)\n",
    "\n",
    "print(\"Evaluation NSP HHblits with L2 regularization...\")\n",
    "\n",
    "print(\"\\nCB513\")\n",
    "evaluation(nsp_l2[0], CB513_hhblits)\n",
    "print(\"\\nTS115\")\n",
    "evaluation(nsp_l2[0], TS115_hhblits)\n",
    "print(\"\\nCASP12\")\n",
    "evaluation(nsp_l2[0], CASP12_hhblits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
