{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of NetSurfP 2.0 with pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to replicate the current version of NetSurfP 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different libraries are initialized and pytorch is either configured to use the CPU or an available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# debugging\n",
    "import pdb\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is compressed as a numpy zip and the dimensions of the dataset are as following: [sequence, position, label]\n",
    "\n",
    "The labels are as following in the data:\n",
    "- [0:20] Amino Acids (sparse encoding) (Unknown residues are stored as an all-zero vector)\n",
    "- [20:50] hmm profile\n",
    "- [50] Seq mask (1 = seq, 0 = empty)\n",
    "- [51] Disordered mask (0 = disordered, 1 = ordered)\n",
    "- [52] Evaluation mask (For CB513 dataset, 1 = eval, 0 = ignore)\n",
    "- [53] ASA (isolated)\n",
    "- [54] ASA (complexed)\n",
    "- [55] RSA (isolated)\n",
    "- [56] RSA (complexed)\n",
    "- [57:65] Q8 GHIBESTC (Q8 -> Q3: HHHEECCC)\n",
    "- [65:67] Phi+Psi\n",
    "- [67] ASA_max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hhblits = np.load(\"../data/nsp2/training_data/Train_HHblits_small.npz\")\n",
    "CB513_hhblits = np.load(\"../data/nsp2/training_data/CB513_HHblits.npz\")\n",
    "TS115_hhblits = np.load(\"../data/nsp2/training_data/TS115_HHblits.npz\")\n",
    "CASP12_hhblits = np.load(\"../data/nsp2/training_data/CASP12_HHblits_small.npz\")\n",
    "\n",
    "train_mmseqs = np.load(\"../data/nsp2/training_data/Train_MMseqs.npz\")\n",
    "CB513_mmseqs = np.load(\"../data/nsp2/training_data/CB513_MMseqs.npz\")\n",
    "TS115_mmseqs = np.load(\"../data/nsp2/training_data/TS115_MMseqs.npz\")\n",
    "CASP12_mmseqs = np.load(\"../data/nsp2/training_data/CASP12_MMseqs.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dataset(s) subset for quick testing (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_SMALL_DATASET = False\n",
    "\n",
    "if CREATE_SMALL_DATASET:\n",
    "    np.savez(\"../data/nsp2/training_data/Train_HHblits_small.npz\", data=train_hhblits['data'][:5000, :, :])\n",
    "    np.savez(\"../data/nsp2/training_data/CASP12_HHblits_small.npz\", data=CASP12_hhblits['data'][:5000, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data loader class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data loader class is created to load the NSP data into a DataLoader pyTorch. The DataLoader class is used later in the training and evaluation of the NSP model. In the NSPData class, the the data is divided into training and test and converted into a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSPData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            X (np.array): The array that contains the training data\n",
    "            y (np.array): The array that contains the test data\n",
    "        \"\"\"\n",
    "        self.data = torch.tensor(X).float()\n",
    "        self.targets = torch.tensor(y).float()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns train and test data at an index\n",
    "        Args:\n",
    "            index (int): Index at the array\n",
    "        \"\"\"\n",
    "        return self.data[index], self.targets[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the data\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instanciate data into the data loader class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded data from the datasets is used with the DataLoader class and the custom data loader class. Moreover the batch size is set. The data loader will then pull out sequences based on the size of the batch size for training and evaluating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hhblits = DataLoader(NSPData(X=train_hhblits['data'][:, :, :50], \\\n",
    "                                   y=train_hhblits['data'][:, :, 50:68]), \\\n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASP12_hhblits = DataLoader(NSPData(X=CASP12_hhblits['data'][:, :, :50], \\\n",
    "                                    y=CASP12_hhblits['data'][:, :, 50:68]), \\\n",
    "                                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The NSP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer consists of one-hot encoded sequences (20 features amino acids) and a full HMM profile (20 features for amino acid profile, 7 features of state transition probabilities and 3 features for local alignment diversity) The input layer is followed by two layers of 1D CNN layers, that consist of 32 filters with a size of 129 and 257. Whereas the output of the last 1D CNN is concatenated with the initial input features. These residuals connections are used to achieve a better backpropagation without gradient degradation and a deeper network. The concatenated input+residuals is then applied to 2 bidirectional long  short-term memory (LSTM) layers with 1024 nodes, that outputs 2048 hidden neurons. The hidden neurons output is input to a fully connected (FC) layer to predict the 18 classes (RSA, SS8, SS3, φ, ψ, and disorder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSP_Classifier(nn.Module):\n",
    "    def __init__(self, n_init_channels, n_class, n_hidden):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            n_init_channels (int): The size of the incoming feature vector\n",
    "            n_classes (int): The size of the output prediction vector\n",
    "            n_hidden: (int) The amount of hidden neurons in the bidirectional lstm\n",
    "        \"\"\"\n",
    "        super(NSP_Classifier, self).__init__()\n",
    "        \n",
    "        # residual block\n",
    "        self.conv1 = nn.Conv1d(in_channels=n_init_channels, out_channels=32, kernel_size=4, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=4, padding=2)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_init_channels+64, hidden_size=n_hidden, \\\n",
    "                                 num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=n_hidden*2, out_features=n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forwarding of the classifier input\n",
    "        Args:\n",
    "            x (torch.sensor): input data\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate the residuals\n",
    "        x = x.permute(0,2,1)\n",
    "        r = F.relu(self.conv1(x))\n",
    "        r = F.relu(self.conv2(r))\n",
    "                \n",
    "        # concatenate channels from residuals and input\n",
    "        x = torch.cat([r, x],dim=1)\n",
    "        \n",
    "        # calculate double layer bidirectional lstm\n",
    "        x = x.permute(0,2,1)\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # hidden neurons to classes\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instanciate model, loss functions and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NSP Classifier model is instanciated with chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_channels = 50\n",
    "output_classes = 18\n",
    "hidden_neurons = 128 #1024\n",
    "\n",
    "nsp_classifier = NSP_Classifier(initial_channels, output_classes, hidden_neurons)\n",
    "\n",
    "# enable cuda if possible\n",
    "if device.type != \"cpu\":\n",
    "    nsp_classifier.cuda(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different class are extracted from the labels and inputs given to the defined loss function to calculate the loss. The class SS8, SS3 and disorder does use cross entropy loss function and RSA, φ and ψ does use mean squared error loss.\n",
    "\n",
    "The prediction classes are in this following order:\n",
    "- [0:8] = Q8\n",
    "- [8:11] = Q3\n",
    "- [11:12] = RSA\n",
    "- [12:14] = Phi\n",
    "- [14:16] = Psi\n",
    "- [16:18] = Disorder\n",
    "\n",
    "Missing:\n",
    "- <font color='red'>Weights were adjusted so each loss contribution was approximately equal and then fine-tuned for maximum overall performance.</font>\n",
    "\n",
    "- <font color='red'>When the target value for a feature of a given residue was missing, for example, for secondary structure of disordered residues, or φ angles of N-terminal residues, the loss for that output was set to 0.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_loss(inputs, labels):\n",
    "    # permute for cross entropy loss input shape\n",
    "    inputs = inputs.permute(0,2,1)\n",
    "    \n",
    "    # apply the Q8 loss\n",
    "    SS8_labels = torch.argmax(labels[:, :, 7:15], dim=2)\n",
    "    SS8_inputs = inputs[:, 0:8, :]\n",
    "\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")(SS8_inputs, SS8_labels)\n",
    "    \n",
    "    # apply the SS3 loss\n",
    "    SS3_mask = torch.tensor([0,0,0,1,1,2,2,2], device=device)\n",
    "    \n",
    "    SS3_labels = torch.max(labels[:, :, 7:15] * SS3_mask, dim=2)[0].long()\n",
    "    SS3_inputs = inputs[:, 8:11, :]\n",
    "    \n",
    "    loss += nn.CrossEntropyLoss(reduction=\"none\")(SS3_inputs, SS3_labels)\n",
    "    \n",
    "    # apply the disorder loss\n",
    "    disorder_labels = labels[:, :, 1].long()\n",
    "    disorder_inputs = inputs[:, 16:18, :]\n",
    "    \n",
    "    loss += nn.CrossEntropyLoss(reduction=\"none\")(disorder_inputs, disorder_labels)\n",
    "    \n",
    "    # permute back to original shape\n",
    "    inputs = inputs.permute(0,2,1)\n",
    "    \n",
    "    # apply RSA loss\n",
    "    rsa_labels = labels[:, :, 6]\n",
    "    rsa_inputs = inputs[:, :, 11]\n",
    "    \n",
    "    loss += nn.MSELoss(reduction=\"none\")(rsa_inputs, rsa_labels)\n",
    "    \n",
    "    # apply phi loss\n",
    "    phi_labels = labels[:, :, 15].unsqueeze(2)\n",
    "    phi_inputs = inputs[:, :, 12:14]\n",
    "    \n",
    "    loss += torch.sum(nn.MSELoss(reduction=\"none\")(phi_inputs, \\\n",
    "                        torch.cat((torch.sin(phi_labels), torch.cos(phi_labels)),dim=2)), dim=2)\n",
    "    \n",
    "    # apply psi loss\n",
    "    psi_labels = labels[:, :, 16].unsqueeze(2)\n",
    "    psi_inputs = inputs[:, :, 14:16]\n",
    "    \n",
    "    loss += torch.sum(nn.MSELoss(reduction=\"none\")(psi_inputs, \\\n",
    "                         torch.cat((torch.sin(psi_labels), torch.cos(psi_labels)),dim=2)), dim=2)\n",
    "        \n",
    "    #only use loss from true sequences\n",
    "    loss = torch.mean(loss[labels[:, :, 0] == 1])\n",
    "        \n",
    "    return loss\n",
    "    \n",
    "optimizer = optim.Adam(nsp_classifier.parameters(), lr=0.001,betas=(0.85,0.95),weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained by iteration through the complete training dataset for an amount of epochs, where its split into multiple batches. The model is trained using backpropagation with the calculated loss for the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  of  2\n",
      "[1,    25] loss: 5.219\n",
      "[1,    50] loss: 4.990\n",
      "[1,    75] loss: 4.671\n",
      "[1,   100] loss: 4.277\n",
      "[1,   125] loss: 4.125\n",
      "[1,   150] loss: 3.959\n",
      "[1,   175] loss: 3.928\n",
      "[1,   200] loss: 3.903\n",
      "[1,   225] loss: 3.882\n",
      "[1,   250] loss: 3.845\n",
      "[1,   275] loss: 3.732\n",
      "[1,   300] loss: 3.746\n",
      "[1,   325] loss: 3.723\n",
      "Epoch: 2  of  2\n",
      "[2,    25] loss: 3.805\n",
      "[2,    50] loss: 3.825\n",
      "[2,    75] loss: 3.715\n",
      "[2,   100] loss: 3.666\n",
      "[2,   125] loss: 3.720\n",
      "[2,   150] loss: 3.640\n",
      "[2,   175] loss: 3.621\n",
      "[2,   200] loss: 3.636\n",
      "[2,   225] loss: 3.653\n",
      "[2,   250] loss: 3.625\n",
      "[2,   275] loss: 3.548\n",
      "[2,   300] loss: 3.566\n",
      "[2,   325] loss: 3.560\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 2 #50\n",
    "\n",
    "# iterate over the dataset multiple times\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch:', epoch + 1,' of ', epochs)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_hhblits, 0):\n",
    "        inputs, labels = data\n",
    "        # move data tensors to GPU if possible\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = nsp_classifier(inputs)\n",
    "        \n",
    "        # backpropagation by loss function\n",
    "        loss = adjusted_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # running loss\n",
    "        running_loss += loss.item()\n",
    "        if i % 25 == 24:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 25))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(pred, labels):\n",
    "    \"\"\" False positive rate\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted binary numeric values\n",
    "        labels (1D Tensor): vector with correct binary numeric values\n",
    "    \"\"\"\n",
    "    fp = sum((pred == 1) & (labels == 0))\n",
    "    tn = sum((pred == 0) & (labels == 0))\n",
    "    \n",
    "    return (fp/(fp+tn)).item()\n",
    "\n",
    "def mcc(pred, labels):\n",
    "    \"\"\" Mathews correlation coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted binary numeric values\n",
    "        labels (1D Tensor): vector with correct binary numeric values\n",
    "    \"\"\"\n",
    "    fp = sum((pred == 1) & (labels == 0))\n",
    "    tp = sum((pred == 1) & (labels == 1))\n",
    "    fn = sum((pred == 0) & (labels == 1))\n",
    "    tn = sum((pred == 0) & (labels == 0))\n",
    "    \n",
    "    return ((tp*tn-fp*fn)/torch.sqrt(((tp+fp)*(fn+tn)*(tp+fn)*(fp+tn)).float())).item()\n",
    "\n",
    "def pcc(pred, labels):\n",
    "    \"\"\" Pearson correlation coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted numeric values\n",
    "        labels (1D Tensor): vector with correct numeric values\n",
    "    \"\"\"\n",
    "    x = pred - torch.mean(pred)\n",
    "    y = labels - torch.mean(labels)\n",
    "    \n",
    "    return (torch.sum(x * y) / (torch.sqrt(torch.sum(x ** 2)) * torch.sqrt(torch.sum(y ** 2)))).item()\n",
    "\n",
    "def mae(pred, labels):\n",
    "    \"\"\" Mean absolute error\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted numeric values\n",
    "        labels (1D Tensor): vector with correct numeric values\n",
    "    \"\"\"\n",
    "    return (sum(abs(labels - pred))/len(labels)).item()\n",
    "    \n",
    "def accuracy(pred, labels):\n",
    "    \"\"\" Accuracy coefficient\n",
    "    Args:\n",
    "        inputs (1D Tensor): vector with predicted integer values\n",
    "        labels (1D Tensor): vector with correct integer values\n",
    "    \"\"\"\n",
    "    \n",
    "    return (sum((pred == labels)) / len(labels)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seperate dataset is used to evaluate the model. Here is each class evaluated and the prediction benchmarks for the trained model are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSA (PCC): 0.47\n",
      "SS8 [Q8]: 0.62\n",
      "SS3 [Q3]: 0.77\n",
      "Disorder (MCC): 0.52\n",
      "Disorder (FPR): 0.544\n",
      "Phi (MAE): 73.2\n",
      "Psi (MAE): 79.36\n"
     ]
    }
   ],
   "source": [
    "# store evaluation results\n",
    "results = {\n",
    "    \"SS8\": 0,\n",
    "    \"SS3\": 0,\n",
    "    \"disorder_mcc\": 0,\n",
    "    \"disorder_fpr\": 0,\n",
    "    \"RSA\": 0,\n",
    "    \"psi\": 0,\n",
    "    \"phi\": 0,\n",
    "}\n",
    "\n",
    "# iterate through the evaluation dataset\n",
    "with torch.no_grad():\n",
    "    for data in CASP12_hhblits:\n",
    "        # move data tensors to GPU if possible\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        predictions = nsp_classifier(inputs) # predict values\n",
    "        \n",
    "        # Use only true sequences from seq mask\n",
    "        predictions = predictions[labels[:, :, 0] == 1]\n",
    "        labels = labels[labels[:, :, 0] == 1]\n",
    "        \n",
    "        # calculate the SS8 accuracy\n",
    "        SS8_labels = torch.argmax(labels[:, 7:15], dim=1).flatten()\n",
    "        SS8_pred = torch.argmax(predictions[:, 0:8], dim=1).flatten()\n",
    "        \n",
    "        results[\"SS8\"] += accuracy(SS8_pred, SS8_labels)\n",
    "        \n",
    "        # calculate the SS3 accuracy\n",
    "        SS3_mask = torch.tensor([0,0,0,1,1,2,2,2], device=device)\n",
    "        \n",
    "        SS3_labels = torch.max(labels[:, 7:15] * SS3_mask, dim=1)[0].long().flatten()\n",
    "        SS3_pred = torch.argmax(predictions[:, 8:11], dim=1).flatten()\n",
    "        \n",
    "        results[\"SS3\"] += accuracy(SS3_pred, SS3_labels)\n",
    "        \n",
    "        # calculate the FPR of disorder\n",
    "        disorder_labels = labels[:, 1].flatten()\n",
    "        disorder_pred = torch.argmax(predictions[:, 16:18], dim=1).flatten()\n",
    "        \n",
    "        results[\"disorder_mcc\"] += mcc(disorder_pred, disorder_labels)\n",
    "        results[\"disorder_fpr\"] += fpr(disorder_pred, disorder_labels)\n",
    "        \n",
    "        # calculate the RSA\n",
    "        rsa_labels = labels[:, 6].flatten()\n",
    "        rsa_pred = predictions[:, 11].flatten()\n",
    "\n",
    "        results['RSA'] += pcc(rsa_pred, rsa_labels)\n",
    "        \n",
    "        # calculate the psi\n",
    "        phi_labels = labels[:, 15].flatten()\n",
    "        phi_pred = torch.atan2(predictions[:, 12], predictions[:, 13]).flatten()\n",
    "        \n",
    "        results['psi'] += mae(phi_pred, phi_labels)\n",
    "\n",
    "        # calculate the psi\n",
    "        psi_labels = labels[:, 16].flatten()\n",
    "        psi_pred = torch.atan2(predictions[:, 14], predictions[:, 15]).flatten()\n",
    "        \n",
    "        results['phi'] += mae(psi_pred, psi_labels)\n",
    "        \n",
    "    # average prediction results \n",
    "    for method, _ in results.items():\n",
    "        results[method] /= len(CASP12_hhblits)\n",
    "\n",
    "print(\"RSA (PCC): {}\".format(round(results[\"RSA\"], 2)))\n",
    "print(\"SS8 [Q8]: {}\".format(round(results[\"SS8\"], 2)))\n",
    "print(\"SS3 [Q3]: {}\".format(round(results[\"SS3\"], 2)))\n",
    "print(\"Disorder (MCC): {}\".format(round(results[\"disorder_mcc\"], 2)))\n",
    "print(\"Disorder (FPR): {}\".format(round(results[\"disorder_fpr\"], 3)))\n",
    "print(\"Phi (MAE): {}\".format(round(results[\"phi\"], 2)))\n",
    "print(\"Psi (MAE): {}\".format(round(results[\"psi\"], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
