{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "isolated-permit",
   "metadata": {
    "papermill": {
     "duration": 0.015817,
     "end_time": "2021-02-24T22:20:01.307666",
     "exception": false,
     "start_time": "2021-02-24T22:20:01.291849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TransProt Embeddings for NetsurfP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-reader",
   "metadata": {
    "papermill": {
     "duration": 0.011538,
     "end_time": "2021-02-24T22:20:01.332877",
     "exception": false,
     "start_time": "2021-02-24T22:20:01.321339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook will try using embeddings from ProtTrans that provides state of the art pretrained language models for proteins.\n",
    "\n",
    "https://github.com/agemagician/ProtTrans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-train",
   "metadata": {
    "papermill": {
     "duration": 0.011534,
     "end_time": "2021-02-24T22:20:01.355885",
     "exception": false,
     "start_time": "2021-02-24T22:20:01.344351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Load libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "temporal-acceptance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T22:20:01.384410Z",
     "iopub.status.busy": "2021-02-24T22:20:01.383706Z",
     "iopub.status.idle": "2021-02-24T22:20:38.136029Z",
     "shell.execute_reply": "2021-02-24T22:20:38.135645Z"
    },
    "papermill": {
     "duration": 36.767813,
     "end_time": "2021-02-24T22:20:38.136134",
     "exception": false,
     "start_time": "2021-02-24T22:20:01.368321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "import gc\n",
    "import pdb\n",
    "\n",
    "#pip install sentencepiece\n",
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-portugal",
   "metadata": {
    "papermill": {
     "duration": 0.011538,
     "end_time": "2021-02-24T22:20:38.159021",
     "exception": false,
     "start_time": "2021-02-24T22:20:38.147483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-launch",
   "metadata": {
    "papermill": {
     "duration": 0.011295,
     "end_time": "2021-02-24T22:20:38.182411",
     "exception": false,
     "start_time": "2021-02-24T22:20:38.171116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The data is loaded and converted back to aminoacids for the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "filled-parliament",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T22:20:38.209181Z",
     "iopub.status.busy": "2021-02-24T22:20:38.208786Z",
     "iopub.status.idle": "2021-02-24T22:20:38.272046Z",
     "shell.execute_reply": "2021-02-24T22:20:38.271708Z"
    },
    "papermill": {
     "duration": 0.07859,
     "end_time": "2021-02-24T22:20:38.272126",
     "exception": false,
     "start_time": "2021-02-24T22:20:38.193536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_path = \"/home/projects/ht3_aim/people/erikie/nsp/data/nsp2/training_data/\"\n",
    "\n",
    "datasets = [\n",
    "    (\"train_hhblits\", np.load(dir_path + \"Train_HHblits.npz\")),\n",
    "    (\"CB513_hhblits\", np.load(dir_path + \"CB513_HHblits.npz\")),\n",
    "    (\"TS115_hhblits\", np.load(dir_path + \"TS115_HHblits.npz\")),\n",
    "    (\"CASP12_HHblits\", np.load(dir_path + \"CASP12_HHblits.npz\")),\n",
    "    \n",
    "    (\"Train_MMseqs\", np.load(dir_path + \"Train_MMseqs.npz\")),\n",
    "    (\"CB513_MMseqs\", np.load(dir_path + \"CB513_MMseqs.npz\")),\n",
    "    (\"TS115_MMseqs\", np.load(dir_path + \"TS115_MMseqs.npz\")),\n",
    "    (\"CASP12_MMseqs\", np.load(dir_path + \"CASP12_MMseqs.npz\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-flooring",
   "metadata": {
    "papermill": {
     "duration": 0.012191,
     "end_time": "2021-02-24T22:20:38.295737",
     "exception": false,
     "start_time": "2021-02-24T22:20:38.283546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A function converts sparse encoding back to amino acid sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "emotional-occupation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T22:20:38.323656Z",
     "iopub.status.busy": "2021-02-24T22:20:38.323271Z",
     "iopub.status.idle": "2021-02-24T22:20:38.325266Z",
     "shell.execute_reply": "2021-02-24T22:20:38.324936Z"
    },
    "papermill": {
     "duration": 0.01806,
     "end_time": "2021-02-24T22:20:38.325343",
     "exception": false,
     "start_time": "2021-02-24T22:20:38.307283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sparse_to_sequence(dataset):\n",
    "    data = []\n",
    "\n",
    "    aa_decode = np.array([\"N\",\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"V\",\"W\",\"Y\"])\n",
    "\n",
    "    dataset = dataset['data']\n",
    "    # get the amino acid encoding and apply decode mask\n",
    "    for seq_id in range(dataset.shape[0]):\n",
    "        seq_mask = dataset[seq_id, dataset[seq_id, :, 50] == 1, :20]\n",
    "        aa_idx = np.argmax(seq_mask, axis=1)\n",
    "\n",
    "        aa_sequence = str()\n",
    "        for idx in aa_idx:\n",
    "            aa_sequence += aa_decode[idx]\n",
    "\n",
    "        # store decoded sequence\n",
    "        data.append(aa_sequence)\n",
    "        \n",
    "    # map rarely occured amino acids (U,Z,O,B) to (X)\n",
    "    data = [\" \".join(re.sub(r\"[UZOB]\", \"X\", sequence)) for sequence in data]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-strategy",
   "metadata": {
    "papermill": {
     "duration": 0.01113,
     "end_time": "2021-02-24T22:20:38.347934",
     "exception": false,
     "start_time": "2021-02-24T22:20:38.336804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Display part of first sequence to check conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brazilian-motion",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2021-02-24T22:20:38.359567",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L I S N W H N I P Q P H R E T I R G E R Q P K D D Q K F K H D T P N N H K R Q T F C F S P C M K R F '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_to_sequence(datasets[0][1])[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-fundamentals",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## ProtTrans Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-resort",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The pre-trained model is instantiated and a tokenizer to convert the sequence to input data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-hindu",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/home/projects/ht3_aim/people/erikie/nsp/models/prot_t5_xl_bfd\", do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"/home/projects/ht3_aim/people/erikie/nsp/models/prot_t5_xl_bfd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-married",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Load the model to GPU if possible and switch to eval mode to not train on the weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-overhead",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Try to move model to GPU if possible\n",
    "try:\n",
    "    model = model.to(device)\n",
    "except RuntimeError:\n",
    "    device = 'cpu'\n",
    "    model = model.to(device)\n",
    "    \n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-prime",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embeddings(data, batches = 5):\n",
    "    result = torch.tensor([])\n",
    "    \n",
    "    # tokenize the sequences\n",
    "    ids = tokenizer.batch_encode_plus(data, add_special_tokens=True, padding=True)\n",
    "\n",
    "    input_ids = torch.tensor(ids['input_ids'])\n",
    "    attention_mask = torch.tensor(ids['attention_mask'])\n",
    "    \n",
    "    for batch in range(0, len(data), batches):\n",
    "\n",
    "        # extract the embeddings\n",
    "        with torch.no_grad():\n",
    "            embedding = model(input_ids = input_ids[batch:batch+batches].to(device),\n",
    "                              attention_mask = attention_mask[batch:batch+batches].to(device))\n",
    "\n",
    "        # keep the last hidden state and move to CPU\n",
    "        embedding = embedding.last_hidden_state.cpu()\n",
    "        \n",
    "        result = torch.cat([result, embedding], dim=0)\n",
    "        \n",
    "    # add extrapolated zeros\n",
    "    for idx_seq in range(len(data)):\n",
    "        result[idx_seq, len(data[idx_seq]):, :] = 0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-intersection",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Save embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-statistics",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The embeddings are merged with the labels from the original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-mailing",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_embedding(name, dataset):\n",
    "    \n",
    "    # create embedding\n",
    "    result = embeddings(sparse_to_sequence(dataset))\n",
    "    \n",
    "    dataset = torch.tensor(dataset['data'])\n",
    "    result = result[:, 1:result.shape[1]+1, :]\n",
    "    \n",
    "    #merge labels from original dataset and save\n",
    "    result = torch.cat([dataset, result], dim=2)\n",
    "    np.savez_compressed(dir_path + \"prot_t5_xl_bfd_\" + name + \".npz\", data=result.numpy())\n",
    "    \n",
    "    print(name + \" saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-bench",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Add embeddings to datasets and save to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-poland",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, data in datasets:\n",
    "    add_embedding(name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-hello",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "embeddings_transprot.ipynb",
   "output_path": "embeddings_transprot.ipynb",
   "parameters": {},
   "start_time": "2021-02-24T22:18:16.861084",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
